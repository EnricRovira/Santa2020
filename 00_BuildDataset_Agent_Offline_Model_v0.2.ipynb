{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# 1. Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# 2. Global Variables\n",
    "\n",
    "\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# 3. Classes\n",
    "\n",
    "class UtilsEnvironment:\n",
    "    \n",
    "    def __init__(self, path_output, name_experiment=''):\n",
    "        self.path_output = path_output\n",
    "        self.name_experiment = name_experiment\n",
    "        self.dataset = pd.DataFrame()\n",
    "        self.num_bandits = 100\n",
    "        self.iniListsFeatures()\n",
    "         \n",
    "    def iniListsFeatures(self):\n",
    "        self.list_count = []\n",
    "        self.list_count_opp = []\n",
    "        self.list_count_consec_opp = []\n",
    "        self.list_wins = []\n",
    "        self.list_loss = []\n",
    "        self.list_beta = []\n",
    "        self.list_beta_std = []\n",
    "        self.list_binom = []\n",
    "        self.list_qwins = []\n",
    "        self.list_target_probs = []\n",
    "        \n",
    "    def addRecord(self, step, agent, environment):\n",
    "        dict_df_tmp = {}\n",
    "        \n",
    "        arr_values = (\n",
    "                agent.arr_wins - agent.arr_loss    \n",
    "                + agent.arr_count_opp            \n",
    "                - (agent.arr_count_opp>0)*1.5   \n",
    "                + agent.arr_consecutive_opp \n",
    "            ) / (agent.arr_wins + agent.arr_loss + agent.arr_count_opp) * agent.arr_max_thresh\n",
    "        \n",
    "        dict_df_tmp['list_count'] = agent.arr_count\n",
    "        dict_df_tmp['list_count_me'] = agent.arr_count_me\n",
    "        dict_df_tmp['list_max_thresh'] = agent.arr_max_thresh\n",
    "        dict_df_tmp['list_count_opp'] = agent.arr_count_opp\n",
    "        dict_df_tmp['list_count_consec_opp'] = agent.arr_consecutive_opp\n",
    "        dict_df_tmp['list_last_pull_opp'] = agent.arr_last_pull_opp\n",
    "        dict_df_tmp['list_vegas'] = arr_values\n",
    "        dict_df_tmp['list_wins'] = agent.arr_wins\n",
    "        dict_df_tmp['list_loss'] = agent.arr_loss\n",
    "        dict_df_tmp['list_pc_wins'] = (agent.arr_wins - 1) / agent.arr_count_me\n",
    "        dict_df_tmp['list_pc_count_me'] = agent.arr_count_me / agent.arr_count\n",
    "        dict_df_tmp['list_beta'] = agent.arr_beta_distribs\n",
    "        dict_df_tmp['list_beta_std'] = agent.arr_beta_distribs_std\n",
    "        dict_df_tmp['list_qwins'] = agent.arr_q_wins\n",
    "        dict_df_tmp['list_id_bandit'] = np.arange(0, self.num_bandits)\n",
    "        dict_df_tmp['list_prob'] = environment.arr_probs    \n",
    "        \n",
    "        #step and experiment vars \n",
    "        dict_df_tmp['experiment'] = np.repeat(self.name_experiment, self.num_bandits)\n",
    "        dict_df_tmp['step'] = np.repeat(step, self.num_bandits)\n",
    "        \n",
    "        df_tmp=pd.DataFrame(dict_df_tmp)\n",
    "        self.dataset = pd.concat([self.dataset, df_tmp], axis=0).reset_index(drop=True)\n",
    "                    \n",
    "    \n",
    "    def dumpDataset(self):      \n",
    "        self.dataset.to_csv(self.path_output + self.name_experiment + '.csv', index=False)\n",
    "        \n",
    "    \n",
    "    \n",
    "class MultiArmedBanditsEnvironment:\n",
    "    \n",
    "    def __init__(self, debug):\n",
    "        self.n_bandits = 100\n",
    "        self.arr_probs = np.random.uniform(size=100)\n",
    "        self.arr_counts = np.zeros(self.n_bandits)\n",
    "        self.cum_reward = [0, 0]\n",
    "        self.cum_prob = [0, 0]\n",
    "        self.list_cum_reward_agent_0, self.list_cum_reward_agent_1 = [], []\n",
    "        self.list_cum_prob_agent_0, self.list_cum_prob_agent_1 = [], []\n",
    "        self.debug = debug        \n",
    "        \n",
    "    def pull(self, step, action_opp, action_me):\n",
    "        if self.debug:\n",
    "            print(f\"Step: {step}\")\n",
    "            print(f\"Prob action Me: {np.round(self.arr_probs[action_me], 4)}\")\n",
    "            print(f\"Prob action Opp: {np.round(self.arr_probs[action_opp], 4)}\")\n",
    "            print('=='*20)\n",
    "            \n",
    "        rn_me = np.random.random()\n",
    "        rn_enemy = np.random.random()\n",
    "        \n",
    "        if rn_me < self.arr_probs[action_me]:\n",
    "            reward_agent_me = 1\n",
    "        else: \n",
    "            reward_agent_me = 0\n",
    "            \n",
    "        if rn_enemy < self.arr_probs[action_opp]:\n",
    "            reward_agent_opp = 1\n",
    "        else: \n",
    "            reward_agent_opp = 0\n",
    "        \n",
    "        self.cum_reward[0]+=reward_agent_opp\n",
    "        self.cum_reward[1]+=reward_agent_me\n",
    "        \n",
    "        self.cum_prob[0]+=self.arr_probs[action_opp]\n",
    "        self.cum_prob[1]+=self.arr_probs[action_me]\n",
    "        \n",
    "        self.list_cum_reward_agent_0.append(self.cum_reward[0])\n",
    "        self.list_cum_reward_agent_1.append(self.cum_reward[1])\n",
    "        \n",
    "        self.list_cum_prob_agent_0.append(self.cum_prob[0])\n",
    "        self.list_cum_prob_agent_1.append(self.cum_prob[1])\n",
    "        \n",
    "        self.arr_counts[action_me] += 1\n",
    "        self.arr_counts[action_opp] += 1\n",
    "        \n",
    "        self.arr_probs[action_me] = self.arr_probs[action_me] * 0.97\n",
    "        self.arr_probs[action_opp] = self.arr_probs[action_opp] * 0.97\n",
    "        \n",
    "        return self.cum_reward[1], self.cum_reward[0], action_me, action_opp, reward_agent_opp\n",
    "    \n",
    "\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67cc148ae5b74b62bcec6c5f372c4be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in true_divide\n",
      "G:\\PROYECTOS\\Kaggle - Santa 2020\\01_agents\\agent_model_offline_regressor_v0_11.py:62: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.arr_beta_skewness = (2*(self.arr_wins-self.arr_loss)*np.sqrt(self.arr_wins+self.arr_loss+1))/((self.arr_wins+self.arr_loss+2)*np.sqrt(self.arr_wins*self.arr_loss))\n",
      "G:\\PROYECTOS\\Kaggle - Santa 2020\\01_agents\\agent_class_vegas_slot_v0_1.py:66: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.arr_beta_skewness = (2*(self.arr_wins-self.arr_loss)*np.sqrt(self.arr_wins+self.arr_loss+1))/((self.arr_wins+self.arr_loss+2)*np.sqrt(self.arr_wins*self.arr_loss))\n",
      "G:\\PROYECTOS\\Kaggle - Santa 2020\\01_agents\\agent_model_offline_regressor_v0_11.py:147: RuntimeWarning: invalid value encountered in true_divide\n",
      "  'list_pc_wins' : (self.arr_wins - 1) / self.arr_count_me,\n",
      "G:\\PROYECTOS\\Kaggle - Santa 2020\\01_agents\\agent_model_offline_regressor_v0_11.py:148: RuntimeWarning: invalid value encountered in true_divide\n",
      "  'list_pc_count_me' : self.arr_count_me / self.arr_count,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "# 4. Game Simulation\n",
    "\n",
    "num_simulations = 5\n",
    "\n",
    "## 4.1 Initialize objects\n",
    "\n",
    "#from agent_class_vegas_slot_v0_1 import AgentRules as AgentOpp\n",
    "from agent_class_vegas_slot_v0_1 import AgentRules as AgentOpp\n",
    "from agent_model_offline_regressor_v0_11 import AgentRules as AgentMe\n",
    "\n",
    "\n",
    "## 4.2 Game\n",
    "for sim in tqdm(range(num_simulations)):\n",
    "    utils = UtilsEnvironment(path_output='../03_Datasets/Dataset_v0_7/', \n",
    "                             name_experiment=f'agent_model_offline_regressor_v0_11_vs_agent_class_vegas_slot_v0_1_1_{sim}')\n",
    "    mba = MultiArmedBanditsEnvironment(debug=False)\n",
    "\n",
    "    agent_opp = AgentOpp(debug=False)\n",
    "    agent_me = AgentMe(debug=False)\n",
    "    \n",
    "    configuration = {'episodeSteps': 2000, 'actTimeout': 0.25, 'runTimeout': 1200, \n",
    "                 'banditCount': 100, 'decayRate': 0.97, 'sampleResolution': 100}\n",
    "    # 0-opp / 1-me\n",
    "    observation_opp = {'remainingOverageTime': 60, 'agentIndex': 0, 'reward': 0, 'step': 0, 'lastActions': []}\n",
    "    observation_me = {'remainingOverageTime': 60, 'agentIndex': 1, 'reward': 0, 'step': 0, 'lastActions': []}\n",
    "    \n",
    "    # simulation\n",
    "    for step in range(2_000):    \n",
    "        if step>=1:\n",
    "            observation_me['reward'] = reward_agent_me\n",
    "            observation_me['step'] = step\n",
    "            observation_me['lastActions'] = [action_opp, action_me]\n",
    "\n",
    "            observation_opp['reward'] = reward_agent_opp\n",
    "            observation_opp['step'] = step\n",
    "            observation_opp['lastActions'] = [action_opp, action_me]\n",
    "\n",
    "\n",
    "        action_me = agent_me.predict(observation_me, configuration)\n",
    "        action_opp = agent_opp.predict(observation_opp, configuration)\n",
    "        \n",
    "        utils.addRecord(step, agent_me, mba)\n",
    "\n",
    "        reward_agent_me, reward_agent_opp, action_me, action_opp, last_rew_agent_opp = mba.pull(step, action_opp, action_me)\n",
    "\n",
    "    utils.dumpDataset()\n",
    "\n",
    "    \n",
    "\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4cd5f6f24145ff86285dd48f8fb2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=36.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "# 5. Join Datasets\n",
    "\n",
    "path_input = '../03_Datasets/Dataset_v0_7/'\n",
    "name_dataset = 'dataset.csv'\n",
    "\n",
    "df_dataset = pd.DataFrame()\n",
    "for file in tqdm(os.listdir(path_input)):\n",
    "    if not os.path.isdir(file) and file.split('.csv')[0][:7]!='dataset':\n",
    "        df_dataset = pd.concat([df_dataset, pd.read_csv(path_input + file)], axis=0).reset_index(drop=True)\n",
    "        \n",
    "df_dataset.to_csv(path_input + name_dataset, index=False)\n",
    "\n",
    "####################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
